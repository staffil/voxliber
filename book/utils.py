# tts ìƒì„± (ë””ë²„ê¹…ìš©)
import os
import traceback
from django.conf import settings
from elevenlabs import ElevenLabs
from uuid import uuid4
from dotenv import load_dotenv
from pydub import AudioSegment
from book.models import VoiceList,VoiceType
from openai import OpenAI

load_dotenv()

ELEVEN_API_KEY = os.getenv('ELEVEN_API_KEY')
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")
GROK_API_KEY=os.getenv("GROK_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)
eleven_client = ElevenLabs(api_key=ELEVEN_API_KEY)
grok_client = OpenAI(
    api_key=GROK_API_KEY,
    base_url="https://api.x.ai/v1"
)

# print("ElevenLabs í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ:", eleven_client)
# print("openAI:" , openAI_client)



def generate_tts(novel_text, voice_id,language_code,speed_value ):
    try:
        # 1ï¸âƒ£ ìž…ë ¥ í™•ì¸
        if not novel_text or not isinstance(novel_text, str):
            raise ValueError("novel_textê°€ ë¹„ì–´ìžˆê±°ë‚˜ ë¬¸ìžì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")

        print("ðŸ”Š TTS ìƒì„± ìš”ì²­")
        print("ðŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´:", len(novel_text))
        print("ðŸ“ í…ìŠ¤íŠ¸ ì¼ë¶€:", novel_text[:200])  # ì•ž 200ê¸€ìžë§Œ ì¶œë ¥
        print("ìŠ¤í”¼ë“œ:",speed_value)

        # 2ï¸âƒ£ ì˜¤ë””ì˜¤ ì €ìž¥ ê²½ë¡œ ì¤€ë¹„
        audio_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
        os.makedirs(audio_dir, exist_ok=True)
        filename = f"response_{uuid4().hex}.mp3"
        audio_path = os.path.join(audio_dir, filename)
        print("ðŸ“‚ ì˜¤ë””ì˜¤ ì €ìž¥ ê²½ë¡œ:", audio_path)

        # 3ï¸âƒ£ ElevenLabs API í˜¸ì¶œ
        audio_stream = eleven_client.text_to_speech.convert(
            voice_id= voice_id,
            model_id="eleven_v3",
            text=novel_text,
            language_code=language_code,
            voice_settings={
                "stability": 0.5,
                "similarity": 0.5,
                "use_speaker_boost": False
            }
        )
        print("âœ… ElevenLabs API í˜¸ì¶œ ì„±ê³µ")
        print("ðŸ–‡ï¸ audio_stream íƒ€ìž…:", type(audio_stream))

        # 4ï¸âƒ£ ìž„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì €ìž¥
        temp_path = audio_path.replace('.mp3', '_temp.mp3')
        with open(temp_path, "wb") as f:
            for chunk in audio_stream:
                f.write(chunk)
        print("ðŸ’¾ ìž„ì‹œ ì˜¤ë””ì˜¤ ì €ìž¥ ì™„ë£Œ:", temp_path)

        # 5ï¸âƒ£ ì†ë„ ì¡°ì ˆ (pydub ì‚¬ìš©)
        try:
            speed_float = float(speed_value)
            speed_float = max(0.5, min(2.0, speed_float))  # 0.5~2.0 ë²”ìœ„ë¡œ ì œí•œ
        except:
            speed_float = 1.0

        print(f"ðŸŽšï¸ ì†ë„ ì¡°ì ˆ: {speed_float}x")

        if abs(speed_float - 1.0) > 0.01:  # ì†ë„ê°€ 1.0ì´ ì•„ë‹ˆë©´ ì¡°ì ˆ
            audio = AudioSegment.from_mp3(temp_path)

            # ì†ë„ ì¡°ì ˆ: frame_rateë¥¼ ë³€ê²½í•˜ê³  ì›ëž˜ëŒ€ë¡œ ë˜ëŒë¦¼
            # speed > 1: ë¹ ë¥´ê²Œ, speed < 1: ëŠë¦¬ê²Œ
            new_frame_rate = int(audio.frame_rate * speed_float)
            audio_adjusted = audio._spawn(audio.raw_data, overrides={'frame_rate': new_frame_rate})
            audio_adjusted = audio_adjusted.set_frame_rate(audio.frame_rate)

            # ìµœì¢… íŒŒì¼ ì €ìž¥
            audio_adjusted.export(audio_path, format="mp3")
            print(f"âœ… ì†ë„ ì¡°ì ˆ ì™„ë£Œ: {speed_float}x")

            # ìž„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_path)
        else:
            # ì†ë„ ì¡°ì ˆ ë¶ˆí•„ìš”ì‹œ ìž„ì‹œ íŒŒì¼ì„ ìµœì¢… íŒŒì¼ë¡œ ì´ë™
            os.rename(temp_path, audio_path)
            print("âœ… ì†ë„ ì¡°ì ˆ ì—†ì´ ì €ìž¥")

        return audio_path

    except Exception as e:
        print("âŒ TTS ìƒì„± ì˜¤ë¥˜ ë°œìƒ:", e)
        traceback.print_exc()  # ðŸ”¹ ì–´ë””ì„œ ì˜¤ë¥˜ ë‚¬ëŠ”ì§€ ìžì„¸ížˆ ì¶œë ¥
        return None


def merge_audio_files(audio_files, pages_text=None):
    """
    ffmpeg concat ê¸°ë°˜ ì˜¤ë””ì˜¤ ë³‘í•© + íƒ€ìž„ìŠ¤íƒ¬í”„ ìœ ì§€
    """
    import os
    import subprocess
    from uuid import uuid4
    from django.conf import settings

    print("ðŸŽµ ì˜¤ë””ì˜¤ í•©ì¹˜ê¸° ì‹œìž‘...")
    print(f"ðŸ“Š ì´ {len(audio_files)}ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼")

    if not audio_files:
        return None, None

    temp_dir = os.path.join(settings.MEDIA_ROOT, "audio")
    os.makedirs(temp_dir, exist_ok=True)

    concat_list_path = os.path.join(temp_dir, f"concat_{uuid4().hex}.txt")
    output_path = os.path.join(temp_dir, f"merged_{uuid4().hex}.mp3")

    timestamps_info = []
    cumulative_time = 3000  # intro silence ê¸°ì¤€

    # ì¹¨ë¬µ íŒŒì¼ ì¤€ë¹„
    intro_silence = os.path.join(temp_dir, "intro_3000ms.mp3")
    middle_silence = os.path.join(temp_dir, "middle_500ms.mp3")
    outro_silence = os.path.join(temp_dir, "outro_3000ms.mp3")

    def create_silence(duration_ms, path):
        if os.path.exists(path):
            return
        subprocess.run([
            "ffmpeg", "-y",
            "-f", "lavfi",
            "-i", "anullsrc=r=44100:cl=stereo",
            "-t", str(duration_ms / 1000),
            "-q:a", "9",
            path
        ], check=True)

    create_silence(3000, intro_silence)
    create_silence(500, middle_silence)
    create_silence(3000, outro_silence)

    with open(concat_list_path, "w", encoding="utf-8") as f:
        f.write(f"file '{intro_silence}'\n")

        for idx, audio_file in enumerate(audio_files):
            temp_audio_path = os.path.join(temp_dir, f"voice_{uuid4().hex}.mp3")

            audio_file.seek(0)
            if hasattr(audio_file, "chunks"):
                with open(temp_audio_path, "wb") as out:
                    for chunk in audio_file.chunks():
                        out.write(chunk)
            else:
                with open(temp_audio_path, "wb") as out:
                    out.write(audio_file.read())

            duration_sec = float(subprocess.check_output([
                "ffprobe",
                "-v", "error",
                "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1",
                temp_audio_path
            ]).decode().strip())

            duration_ms = int(duration_sec * 1000)

            if idx > 0:
                cumulative_time += 500
                f.write(f"file '{middle_silence}'\n")

            start_time = cumulative_time
            cumulative_time += duration_ms

            timestamps_info.append({
                "pageIndex": idx,
                "startTime": start_time,
                "endTime": cumulative_time,
                "text": pages_text[idx] if pages_text and idx < len(pages_text) else None
            })

            f.write(f"file '{temp_audio_path}'\n")

        f.write(f"file '{outro_silence}'\n")

    subprocess.run([
        "ffmpeg", "-y",
        "-f", "concat",
        "-safe", "0",
        "-i", concat_list_path,
        "-c", "copy",
        output_path
    ], check=True)

    print(f"ðŸŽ‰ ìµœì¢… ì˜¤ë””ì˜¤ ì €ìž¥ ì™„ë£Œ: {output_path}")
    return output_path, timestamps_info



# ì‚¬ìš´ë“œ íš¨ê³¼
def sound_effect(effect_name, effect_description, duration_seconds):
    """
    ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„± í•¨ìˆ˜
    effect_name: ì´íŒ©íŠ¸ ì´ë¦„
    effect_description: ì´íŒ©íŠ¸ ì„¤ëª…
    """

    try:
        print(f"ðŸŽµ ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„±: {effect_name} - {effect_description}")

        detailed_prompt=openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an expert sound designer. Convert the user's short Korean description "
                        "in the prompt must input the sound  first sentence"
                        "into a highly detailed, professional English prompt optimized for ElevenLabs "
                        "sound-effects generation.\n"
                        "Describe:\n"
                        "- sound source and physical characteristics\n"
                        "- environment and ambience\n"
                        "- acoustic texture (reverb, distance, resonance)\n"
                        "- emotional tone and pacing\n"
                        "Keep it under 3 sentences."
                    )
                },
                {"role": "user", "content": effect_description}
            ],
            max_tokens=120 
        )

        effect_prompt = detailed_prompt.choices[0].message.content.strip()
        effect_prompt = effect_prompt[:440]
        print("ai ê°€ ìƒì„±í•œ ì‚¬ìš´ë“œ ì´íŽ™íŠ¸:", effect_prompt)
        audio_stream = eleven_client.text_to_sound_effects.convert(
            text=effect_prompt,
            duration_seconds=duration_seconds,  # ìžë™ ê¸¸ì´
            prompt_influence=1.0
        )

        print("âœ… ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„± ì™„ë£Œ")
        return audio_stream

    except Exception as e:
        print(f"âŒ ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return None
    

import requests
import traceback

# ë°°ê²½ìŒ

def background_music(music_name, music_description, duration_seconds=30):
    """
    ë°°ê²½ìŒ ìƒì„± í•¨ìˆ˜ (REST API ê¸°ë°˜)
    music_name: ìŒì•… ì´ë¦„
    music_description: ìŒì•… ì„¤ëª…
    duration_seconds: ìŒì•… ê¸¸ì´ (ì´ˆ)
    """
    try:
        print(f"ðŸŽµ ë°°ê²½ìŒ ìƒì„±: {music_name} - {music_description} ({duration_seconds}ì´ˆ)")

        url = "https://api.elevenlabs.io/v1/music/generate"
        detailed_prompt = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "in the prompt must input the sound  first sentence"

                        "You are a professional music director creating concise prompts for AI-generated background music. "
                        "Rewrite the user's Korean description into a detailed English BGM prompt under 350 characters. "
                        "Describe the mood, instruments, tempo, atmosphere, and acoustic space. "
                        "Keep it to 2 sentences and explicitly say 'no vocals, no lyrics'."
                    )
                },
                {
                    "role": "user",
                    "content": music_description
                }
            ],
            max_tokens=120
        )

        # ê²°ê³¼ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        refined_prompt = detailed_prompt.choices[0].message.content.strip()

        # ì•ˆì „ìž¥ì¹˜: 450ìž ì´ˆê³¼ ì‹œ ìžë™ ìžë¥´ê¸°
        refined_prompt = refined_prompt[:430]
        print("ai ê°€ ìƒì„±í•œ ë°°ê²½ìŒ:", refined_prompt)

        payload = {
            "prompt": refined_prompt,
            "duration_seconds": duration_seconds,
            "generation_settings": {
                "prompt_influence": 1.0,
            }
        }

        headers = {
            "xi-api-key": ELEVEN_API_KEY,
            "Content-Type": "application/json"
        }

        response = requests.post(url, headers=headers, json=payload, stream=True)

        if response.status_code != 200:
            print("âŒ Music API Error:", response.text)
            return None

        print("âœ… ë°°ê²½ìŒ ìƒì„± ì™„ë£Œ")
        return response.iter_content(chunk_size=1024)

    except Exception as e:
        print(f"âŒ ë°°ê²½ìŒ ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return None


# ë°°ê²½ìŒê³¼ ëŒ€ì‚¬ ë¯¹ì‹± í•¨ìˆ˜
def mix_audio_with_background(dialogue_audio_path, background_tracks_info):
    """
    ëŒ€ì‚¬ ì˜¤ë””ì˜¤ì™€ ë°°ê²½ìŒì„ ë¯¹ì‹±í•˜ëŠ” í•¨ìˆ˜
    dialogue_audio_path: í•©ì³ì§„ ëŒ€ì‚¬ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    background_tracks_info: [{audioPath, startTime, endTime, volume}] í˜•íƒœì˜ ë°°ê²½ìŒ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        print("ðŸŽµ ë°°ê²½ìŒ ë¯¹ì‹± ì‹œìž‘...")

        # ëŒ€ì‚¬ ì˜¤ë””ì˜¤ ë¡œë“œ
        dialogue_audio = AudioSegment.from_mp3(dialogue_audio_path)
        dialogue_duration = len(dialogue_audio)
        print(f"ðŸ“Š ëŒ€ì‚¬ ì˜¤ë””ì˜¤ ê¸¸ì´: {dialogue_duration}ms")

        # ë°°ê²½ìŒì´ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not background_tracks_info:
            print("âš ï¸ ë°°ê²½ìŒì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return dialogue_audio_path

        # ê° ë°°ê²½ìŒ íŠ¸ëž™ ì²˜ë¦¬
        for idx, track_info in enumerate(background_tracks_info):
            bg_audio_path = track_info.get('audioPath')
            start_time = track_info.get('startTime', 0)  # ms ë‹¨ìœ„
            end_time = track_info.get('endTime', dialogue_duration)  # ms ë‹¨ìœ„
            volume_adjust = track_info.get('volume', -10)  # dB ë‹¨ìœ„ (ê¸°ë³¸: -10dBë¡œ ë°°ê²½ìŒ ë³¼ë¥¨ ë‚®ì¶¤)

            print(f"ðŸŽ¼ ë°°ê²½ìŒ {idx + 1} ì²˜ë¦¬ ì¤‘...")
            print(f"   - ì‹œìž‘: {start_time}ms, ì¢…ë£Œ: {end_time}ms, ë³¼ë¥¨: {volume_adjust}dB")

            # ë°°ê²½ìŒ ë¡œë“œ
            bg_audio = AudioSegment.from_file(bg_audio_path)

            # ë°°ê²½ìŒ ë³¼ë¥¨ ì¡°ì ˆ
            bg_audio = bg_audio + volume_adjust

            # í•„ìš”í•œ ê¸¸ì´ ê³„ì‚°
            required_duration = end_time - start_time

            # ë°°ê²½ìŒì´ í•„ìš”í•œ ê¸¸ì´ë³´ë‹¤ ì§§ìœ¼ë©´ ë°˜ë³µ
            if len(bg_audio) < required_duration:
                repeat_times = (required_duration // len(bg_audio)) + 1
                bg_audio = bg_audio * repeat_times

            # í•„ìš”í•œ ê¸¸ì´ë§Œí¼ ìžë¥´ê¸°
            bg_audio = bg_audio[:required_duration]

            # íŽ˜ì´ë“œ ì¸/ì•„ì›ƒ íš¨ê³¼ (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
            fade_duration = min(500, required_duration // 4)  # 500ms ë˜ëŠ” ì „ì²´ ê¸¸ì´ì˜ 1/4
            bg_audio = bg_audio.fade_in(fade_duration).fade_out(fade_duration)

            # ë°°ê²½ìŒì„ ì ì ˆí•œ ìœ„ì¹˜ì— ì˜¤ë²„ë ˆì´
            # start_time ìœ„ì¹˜ë¶€í„° bg_audioë¥¼ ë¯¹ì‹±
            dialogue_audio = dialogue_audio.overlay(bg_audio, position=start_time)
            print(f"âœ… ë°°ê²½ìŒ {idx + 1} ë¯¹ì‹± ì™„ë£Œ")

        # ìµœì¢… ë¯¹ì‹±ëœ ì˜¤ë””ì˜¤ ì €ìž¥
        output_filename = f"mixed_{uuid4().hex}.mp3"
        output_path = os.path.join(settings.MEDIA_ROOT, 'audio', output_filename)
        dialogue_audio.export(output_path, format="mp3")

        print(f"âœ… ë°°ê²½ìŒ ë¯¹ì‹± ì™„ë£Œ: {output_path}")
        return output_path

    except Exception as e:
        print(f"âŒ ë°°ê²½ìŒ ë¯¹ì‹± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ëŒ€ì‚¬ ì˜¤ë””ì˜¤ ê²½ë¡œ ë°˜í™˜
        return dialogue_audio_path
    

import os
import requests
from django.utils import timezone
from django.core.files.base import ContentFile
from book.models import VoiceList, VoiceType
from elevenlabs import ElevenLabs

def sync_voices_with_type():
    """
    ElevenLabsì˜ User Voice / Default Voiceë¥¼ DBì— ë„£ê³ ,
    VoiceTypeë„ ì—°ê²°í•˜ë©° sample_audioê¹Œì§€ ì €ìž¥
    """
    ELEVEN_API_KEY = os.getenv('ELEVEN_API_KEY')
    eleven_client = ElevenLabs(api_key=ELEVEN_API_KEY)
    
    try:
        print("ElevenLabs í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ:", eleven_client)

        # 1. VoiceType ìƒì„±
        user_voice_type, _ = VoiceType.objects.get_or_create(name="User Voice")
        default_voice_type, _ = VoiceType.objects.get_or_create(name="Default Voice")

        # 2. ëª¨ë“  ë³´ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        all_voices = eleven_client.voices.get_all().voices
        print(f"ì´ {len(all_voices)}ê°œ ë³´ì´ìŠ¤ ê°€ì ¸ì˜´")

        for v in all_voices:
            voice_id = getattr(v, "voice_id", None)
            if not voice_id:
                print(f"âš ï¸ voice_id ì—†ìŒ, ìŠ¤í‚µ: {getattr(v, 'name', 'unknown')}")
                continue

            # 3. DBì— ì €ìž¥
            voice, created = VoiceList.objects.update_or_create(
                voice_id=voice_id,
                defaults={
                    "voice_name": getattr(v, "name", "Unknown"),
                    "voice_description": getattr(v, "description", ""),
                    "language_code": getattr(v, "language", "en"),
                    "created_at": timezone.now(),
                }
            )

            # 4. íƒ€ìž… ì—°ê²°
            if getattr(v, "is_user", False):
                voice.types.add(user_voice_type)
            else:
                voice.types.add(default_voice_type)

            # 5. sample_audio ì €ìž¥
            preview_url = getattr(v, "preview_url", None)
            if preview_url:
                try:
                    r = requests.get(preview_url, timeout=10)
                    if r.status_code == 200:
                        filename = f"{voice.voice_name}_{voice.voice_id}.mp3".replace(" ", "_")
                        voice.sample_audio.save(filename, ContentFile(r.content), save=True)
                except Exception as e:
                    print(f"âš ï¸ ìƒ˜í”Œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {voice.voice_name}, {e}")

            print(f"{'ìƒì„±' if created else 'ì—…ë°ì´íŠ¸'}: {voice.voice_name}")

        print("âœ… Voice sync ì™„ë£Œ")

    except Exception as e:
        print("âŒ Voice sync ì‹¤íŒ¨:", e)






from openai import OpenAI
from django.conf import settings
import os
from book.models import VoiceList, Books


def chat_with_character(book_id, message):
    """
    GROK(OpenAI Grok API) ê¸°ë°˜ ìºë¦­í„° ëŒ€í™” í•¨ìˆ˜
    """

    # ì±… ë‚´ìš© ë¡œë“œ
    try:
        book = Books.objects.get(id=book_id)
        book_info = book.description or ""
        character_name = book.character_name or "ìºë¦­í„°"
    except:
        book_info = ""
        character_name = "ìºë¦­í„°"

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""
ë‹¹ì‹ ì€ '{character_name}'ë¼ëŠ” ìºë¦­í„°ìž…ë‹ˆë‹¤.

ì•„ëž˜ëŠ” ì±…ì˜ ì„¤ì •ìž…ë‹ˆë‹¤:

{book_info}

ì‚¬ìš©ìž ë©”ì‹œì§€ì— '{character_name}'ì˜ ë§íˆ¬ë¡œ ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”.
ë§íˆ¬ëŠ” ìºë¦­í„° ì„±ê²©ì— ë§žì¶”ê³ , ì¡´ëŒ“ë§/ë°˜ë§ì€ ìƒí™©ì— ë§žê²Œ ìžì—°ìŠ¤ëŸ½ê²Œ ì„ íƒí•˜ì„¸ìš”.

ì‚¬ìš©ìž: {message}
"""

    # --------------------------
    # ðŸ”¥ GROK í˜¸ì¶œ (grok_client)
    # --------------------------
    try:
        completion = grok_client.chat.completions.create(
            model="grok-4",   # ê·¸ë¡ ëª¨ë¸ ì´ë¦„(ë„£ì€ í‚¤ì— ë§žì¶° ë³€ê²½ ê°€ëŠ¥)
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì†Œì„¤ ì† ë“±ìž¥ì¸ë¬¼ì²˜ëŸ¼ ë§í•˜ëŠ” ìºë¦­í„° AIì´ë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        ai_text = completion.choices[0].message.content

    except Exception as e:
        ai_text = f"[GROK ì˜¤ë¥˜] {str(e)}"

    # --------------------------
    # ðŸ”Š TTS ì²˜ë¦¬
    # --------------------------

    voice_id = "WAhoMTNdLdMoq1j3wf3I"
    language_code = "ko"

    audio_path = generate_tts(
        novel_text=ai_text,
        voice_id=voice_id,
        language_code=language_code,
        speed_value=1.0
    )

    # --------------------------
    # URL ë³€í™˜
    # --------------------------
    if audio_path and os.path.exists(str(audio_path)):
        rel_path = os.path.relpath(str(audio_path), settings.MEDIA_ROOT)
        audio_url = settings.MEDIA_URL + rel_path.replace("\\", "/")
    else:
        audio_url = None

    return {
        "text": ai_text,
        "audio": audio_url,
        "debug": {
            "ai_text": ai_text,
            "audio_path": str(audio_path),
            "audio_url": audio_url,
            "character": character_name,
            "source": "grok"
        }
    }



import json
from book.models import Books
# elevenlabs client import í•„ìš” ì‹œ ì¶”ê°€
# from elevenlabs import ElevenLabsClient
# eleven_client = ElevenLabsClient(api_key="YOUR_API_KEY")

def chat_with_character_debug(agent_id, character_name, voice_id, book_content, user_input):
    """
    ë””ë²„ê·¸ìš© ëž˜í¼ í•¨ìˆ˜:
    ì‹¤ì œ ElevenLabs í˜¸ì¶œ ì—†ì´, TTS ë°˜í™˜ ëŒ€ì‹  debugìš© JSON ë°˜í™˜
    """
    if not agent_id or not character_name or not voice_id or not user_input:
        missing = []
        if not agent_id: missing.append("agent_id")
        if not character_name: missing.append("character_name")
        if not voice_id: missing.append("voice_id")
        if not user_input: missing.append("user_input")
        raise ValueError(f"ëˆ„ë½ëœ í•„ìˆ˜ í•„ë“œ: {', '.join(missing)}")

    # ì‹¤ì œ API í˜¸ì¶œì€ ì£¼ì„ ì²˜ë¦¬
    # response = eleven_client.conversational_ai.agents.chat(
    #     agent_id=agent_id,
    #     input=f"ìºë¦­í„° {character_name}, ë‚´ìš©: {book_content}, ì§ˆë¬¸: {user_input}",
    #     tts={"model_id": voice_id}
    # )

    # ë””ë²„ê·¸ìš© ë°˜í™˜
    debug_response = {
        "agent_id": agent_id,
        "character_name": character_name,
        "voice_id": voice_id,
        "book_content_snippet": book_content[:100] if book_content else "",
        "user_input": user_input,
        "audio": f"ì˜¤ë””ì˜¤_ë°ì´í„°_ì‹œë®¬ë ˆì´ì…˜_for_{character_name}"
    }

    return json.dumps(debug_response)

