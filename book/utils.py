# tts ìƒì„± (ë””ë²„ê¹…ìš©)
import os
import traceback
from django.conf import settings
from elevenlabs import ElevenLabs
from uuid import uuid4
from dotenv import load_dotenv
from pydub import AudioSegment
from book.models import VoiceList,VoiceType
from openai import OpenAI

load_dotenv()

ELEVEN_API_KEY = os.getenv('ELEVEN_API_KEY')
OPENAI_API_KEY=os.getenv("OPENAI_API_KEY")
GROK_API_KEY=os.getenv("GROK_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)
eleven_client = ElevenLabs(api_key=ELEVEN_API_KEY)
grok_client = OpenAI(
    api_key=GROK_API_KEY,
    base_url="https://api.x.ai/v1"
)

# print("ElevenLabs í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ:", eleven_client)
# print("openAI:" , openAI_client)



def generate_tts(novel_text, voice_id,language_code,speed_value, style_value, similarity_value ):
    try:
        # 1ï¸âƒ£ ì…ë ¥ í™•ì¸
        if not novel_text or not isinstance(novel_text, str):
            raise ValueError("novel_textê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")

        print("ğŸ”Š TTS ìƒì„± ìš”ì²­")
        print("ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´:", len(novel_text))
        print("ğŸ“ í…ìŠ¤íŠ¸ ì¼ë¶€:", novel_text[:200])  # ì• 200ê¸€ìë§Œ ì¶œë ¥
        print("ìŠ¤í”¼ë“œ:",speed_value)

        # 2ï¸âƒ£ ì˜¤ë””ì˜¤ ì €ì¥ ê²½ë¡œ ì¤€ë¹„
        audio_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
        os.makedirs(audio_dir, exist_ok=True)
        filename = f"response_{uuid4().hex}.mp3"
        audio_path = os.path.join(audio_dir, filename)
        print("ğŸ“‚ ì˜¤ë””ì˜¤ ì €ì¥ ê²½ë¡œ:", audio_path)

        # 3ï¸âƒ£ ElevenLabs API í˜¸ì¶œ
        audio_stream = eleven_client.text_to_speech.convert(
            voice_id= voice_id,
            model_id="eleven_v3",
            text=novel_text,
            language_code=language_code,
            voice_settings={
                "stability": 0.5,
                "similarity": similarity_value,
                "style": style_value,
                "use_speaker_boost": False
            }
        )
        print("âœ… ElevenLabs API í˜¸ì¶œ ì„±ê³µ")
        print("ğŸ–‡ï¸ audio_stream íƒ€ì…:", type(audio_stream))

        # 4ï¸âƒ£ ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥
        temp_path = audio_path.replace('.mp3', '_temp.mp3')
        with open(temp_path, "wb") as f:
            for chunk in audio_stream:
                f.write(chunk)
        print("ğŸ’¾ ì„ì‹œ ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ:", temp_path)

        # 5ï¸âƒ£ ì†ë„ ì¡°ì ˆ (pydub ì‚¬ìš©)
        try:
            speed_float = float(speed_value)
            speed_float = max(0.5, min(2.0, speed_float))  # 0.5~2.0 ë²”ìœ„ë¡œ ì œí•œ
        except:
            speed_float = 1.0

        print(f"ğŸšï¸ ì†ë„ ì¡°ì ˆ: {speed_float}x")

        if abs(speed_float - 1.0) > 0.01:  # ì†ë„ê°€ 1.0ì´ ì•„ë‹ˆë©´ ì¡°ì ˆ
            audio = AudioSegment.from_mp3(temp_path)

            # ì†ë„ ì¡°ì ˆ: frame_rateë¥¼ ë³€ê²½í•˜ê³  ì›ë˜ëŒ€ë¡œ ë˜ëŒë¦¼
            # speed > 1: ë¹ ë¥´ê²Œ, speed < 1: ëŠë¦¬ê²Œ
            new_frame_rate = int(audio.frame_rate * speed_float)
            audio_adjusted = audio._spawn(audio.raw_data, overrides={'frame_rate': new_frame_rate})
            audio_adjusted = audio_adjusted.set_frame_rate(audio.frame_rate)

            # ìµœì¢… íŒŒì¼ ì €ì¥
            audio_adjusted.export(audio_path, format="mp3")
            print(f"âœ… ì†ë„ ì¡°ì ˆ ì™„ë£Œ: {speed_float}x")

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_path)
        else:
            # ì†ë„ ì¡°ì ˆ ë¶ˆí•„ìš”ì‹œ ì„ì‹œ íŒŒì¼ì„ ìµœì¢… íŒŒì¼ë¡œ ì´ë™
            os.rename(temp_path, audio_path)
            print("âœ… ì†ë„ ì¡°ì ˆ ì—†ì´ ì €ì¥")

        return audio_path

    except Exception as e:
        print("âŒ TTS ìƒì„± ì˜¤ë¥˜ ë°œìƒ:", e)
        traceback.print_exc()  # ğŸ”¹ ì–´ë””ì„œ ì˜¤ë¥˜ ë‚¬ëŠ”ì§€ ìì„¸íˆ ì¶œë ¥
        return None

def merge_audio_files(audio_files, pages_text=None):
    """
    ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜ (íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ í¬í•¨)

    Returns:
        tuple: (merged_audio_path, timestamps_info) ë˜ëŠ” (None, None)
        - merged_audio_path: í•©ì³ì§„ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        - timestamps_info: ê° ëŒ€ì‚¬ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    import traceback
    try:
        print("ğŸµ ì˜¤ë””ì˜¤ í•©ì¹˜ê¸° ì‹œì‘...")
        print(f"ğŸ“Š ì´ {len(audio_files)}ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼")

        if not audio_files:
            print("âš ï¸ í•©ì¹  ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None

        # ì„ì‹œ ì €ì¥ í´ë” í™•ì¸
        temp_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
        os.makedirs(temp_dir, exist_ok=True)
        print(f"ğŸ—‚ ì„ì‹œ í´ë” í™•ì¸: {temp_dir}")

        combined = None
        timestamps_info = []
        intro_silence_duration = 3000  # ì‹œì‘ ì¹¨ë¬µ ì‹œê°„ (ms)
        cumulative_time = intro_silence_duration  # ì‹œì‘ ì¹¨ë¬µ ì‹œê°„ë¶€í„° ì‹œì‘

        for idx, audio_file in enumerate(audio_files):
            print(f"ğŸ”„ {idx + 1}/{len(audio_files)} ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì¤‘...")

            temp_path = None
            is_temp_file = False

            # íŒŒì¼ ê²½ë¡œ(ë¬¸ìì—´) ë˜ëŠ” íŒŒì¼ ê°ì²´ ì²˜ë¦¬
            try:
                if isinstance(audio_file, str):
                    # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° - ì§ì ‘ ì‚¬ìš©
                    if os.path.exists(audio_file):
                        temp_path = audio_file
                        is_temp_file = False
                        print(f"ğŸ“‚ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©: {temp_path}")
                    else:
                        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {audio_file}")
                        return None, None, None
                else:
                    # íŒŒì¼ ê°ì²´ì¸ ê²½ìš° - ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    temp_path = os.path.join(temp_dir, f'temp_{uuid4().hex}.mp3')
                    is_temp_file = True
                    audio_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
                    if hasattr(audio_file, 'chunks'):
                        with open(temp_path, 'wb') as f:
                            for chunk in audio_file.chunks():
                                f.write(chunk)
                    else:
                        with open(temp_path, 'wb') as f:
                            f.write(audio_file.read())
                    print(f"ğŸ’¾ ì„ì‹œ íŒŒì¼ ì €ì¥: {temp_path}")
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                traceback.print_exc()
                return None, None, None

            # AudioSegment ë¡œë“œ
            try:
                audio_segment = AudioSegment.from_file(temp_path)
                duration = len(audio_segment)  # ms ë‹¨ìœ„
                print(f"âœ… ë¡œë“œ ì™„ë£Œ: {duration}ms")

                # ì²« ë²ˆì§¸ê°€ ì•„ë‹ˆë©´ ëŒ€ì‚¬ ì‚¬ì´ ì¹¨ë¬µ ì‹œê°„ ì¶”ê°€
                if idx > 0:
                    cumulative_time += 500

                page_start = cumulative_time  # startTime: ì´ í˜ì´ì§€ ì˜¤ë””ì˜¤ ì‹œì‘ ì‹œì 

                # ì˜¤ë””ì˜¤ ê¸¸ì´ë§Œí¼ ëˆ„ì 
                cumulative_time += duration

                timestamp_data = {
                    'pageIndex': idx,
                    'startTime': page_start,
                    'endTime': cumulative_time
                }

                # í˜ì´ì§€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if pages_text and idx < len(pages_text):
                    timestamp_data['text'] = pages_text[idx]

                timestamps_info.append(timestamp_data)

                # ì˜¤ë””ì˜¤ ë³‘í•©
                if combined is None:
                    combined = audio_segment
                else:
                    silence = AudioSegment.silent(duration=500)
                    combined = combined + silence + audio_segment

            except Exception as e:
                print(f"âŒ AudioSegment ë¡œë“œ ì‹¤íŒ¨: {e}")
                traceback.print_exc()
                # ì„ì‹œ íŒŒì¼ë§Œ ì‚­ì œ (ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ íŒŒì¼ ê²½ë¡œëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ)
                if is_temp_file and temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
                return None, None, None

            # ì„ì‹œ íŒŒì¼ë§Œ ì‚­ì œ (ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ íŒŒì¼ ê²½ë¡œëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ)
            if is_temp_file and temp_path and os.path.exists(temp_path):
                os.remove(temp_path)

        # ìµœì¢… ì˜¤ë””ì˜¤ export

        intro_silence = AudioSegment.silent(duration=3000)  # ì›í•˜ëŠ” ê¸¸ì´ ì§€ì •(ms)
        outro_silence  = AudioSegment.silent(duration=3000)  # ì›í•˜ëŠ” ê¸¸ì´ ì§€ì •(ms)
        combined = intro_silence + combined +outro_silence 
        output_filename = f"merged_{uuid4().hex}.mp3"
        output_path = os.path.join(temp_dir, output_filename)
        combined.export(output_path, format="mp3", bitrate="128k")  # ë¹„íŠ¸ë ˆì´íŠ¸ ìµœì í™”
        total_duration = len(combined) / 1000
        print(f"ğŸ‰ ìµœì¢… ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"â±ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ {len(timestamps_info)}ê°œ ìƒì„± ì™„ë£Œ")
        print("ğŸ”¥ RETURNING 3 VALUES")
        return output_path, timestamps_info,total_duration

    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ í•©ì¹˜ê¸° ìµœì¢… ì—ëŸ¬: {e}")
        traceback.print_exc()
        return None, None, None







# ì‚¬ìš´ë“œ íš¨ê³¼
def sound_effect(effect_name, effect_description, duration_seconds):
    """
    ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„± í•¨ìˆ˜
    effect_name: ì´íŒ©íŠ¸ ì´ë¦„
    effect_description: ì´íŒ©íŠ¸ ì„¤ëª…
    """

    try:
        print(f"ğŸµ ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„±: {effect_name} - {effect_description}")

        detailed_prompt=openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are an expert sound designer. Convert the user's short Korean description "
                        "in the prompt must input the sound  first sentence"
                        "into a highly detailed, professional English prompt optimized for ElevenLabs "
                        "sound-effects generation.\n"
                        "Describe:\n"
                        "- sound source and physical characteristics\n"
                        "- environment and ambience\n"
                        "- acoustic texture (reverb, distance, resonance)\n"
                        "- emotional tone and pacing\n"
                        "Keep it under 3 sentences."
                    )
                },
                {"role": "user", "content": effect_description}
            ],
            max_tokens=120 
        )

        effect_prompt = detailed_prompt.choices[0].message.content.strip()
        effect_prompt = effect_prompt[:440]
        print("ai ê°€ ìƒì„±í•œ ì‚¬ìš´ë“œ ì´í™íŠ¸:", effect_prompt)
        audio_stream = eleven_client.text_to_sound_effects.convert(
            text=effect_prompt,
            duration_seconds=duration_seconds,
            prompt_influence=1.0
        )

        # íŒŒì¼ë¡œ ì €ì¥
        audio_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
        os.makedirs(audio_dir, exist_ok=True)
        filename = f"sfx_{uuid4().hex}.mp3"
        audio_path = os.path.join(audio_dir, filename)

        with open(audio_path, 'wb') as f:
            for chunk in audio_stream:
                f.write(chunk)

        print(f"âœ… ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„± ì™„ë£Œ: {audio_path}")
        return audio_path

    except Exception as e:
        print(f"âŒ ì‚¬ìš´ë“œ ì´íŒ©íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return None
    

import requests
import traceback

# ë°°ê²½ìŒ

def background_music(music_name, music_description, duration_seconds=30):
    """
    ë°°ê²½ìŒ ìƒì„± í•¨ìˆ˜ (REST API ê¸°ë°˜)
    music_name: ìŒì•… ì´ë¦„
    music_description: ìŒì•… ì„¤ëª…
    duration_seconds: ìŒì•… ê¸¸ì´ (ì´ˆ)
    """
    try:
        print(f"ğŸµ ë°°ê²½ìŒ ìƒì„±: {music_name} - {music_description} ({duration_seconds}ì´ˆ)")

        url = "https://api.elevenlabs.io/v1/music/generate"
        detailed_prompt = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "in the prompt must input the sound  first sentence"

                        "You are a professional music director creating concise prompts for AI-generated background music. "
                        "Rewrite the user's Korean description into a detailed English BGM prompt under 350 characters. "
                        "Describe the mood, instruments, tempo, atmosphere, and acoustic space. "
                        "Keep it to 2 sentences and explicitly say 'no vocals, no lyrics'."
                    )
                },
                {
                    "role": "user",
                    "content": music_description
                }
            ],
            max_tokens=120
        )

        # ê²°ê³¼ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        refined_prompt = detailed_prompt.choices[0].message.content.strip()

        # ì•ˆì „ì¥ì¹˜: 450ì ì´ˆê³¼ ì‹œ ìë™ ìë¥´ê¸°
        refined_prompt = refined_prompt[:430]
        print("ai ê°€ ìƒì„±í•œ ë°°ê²½ìŒ:", refined_prompt)

        payload = {
            "prompt": refined_prompt,
            "duration_seconds": duration_seconds,
            "generation_settings": {
                "prompt_influence": 1.0,
            }
        }

        headers = {
            "xi-api-key": ELEVEN_API_KEY,
            "Content-Type": "application/json"
        }

        response = requests.post(url, headers=headers, json=payload, stream=True)

        if response.status_code != 200:
            print("âŒ Music API Error:", response.text)
            return None

        # íŒŒì¼ë¡œ ì €ì¥
        audio_dir = os.path.join(settings.MEDIA_ROOT, 'audio')
        os.makedirs(audio_dir, exist_ok=True)
        filename = f"bgm_{uuid4().hex}.mp3"
        audio_path = os.path.join(audio_dir, filename)

        with open(audio_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=1024):
                f.write(chunk)

        print(f"âœ… ë°°ê²½ìŒ ìƒì„± ì™„ë£Œ: {audio_path}")
        return audio_path

    except Exception as e:
        print(f"âŒ ë°°ê²½ìŒ ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return None


# ë°°ê²½ìŒê³¼ ëŒ€ì‚¬ ë¯¹ì‹± í•¨ìˆ˜
def mix_audio_with_background(dialogue_audio_path, background_tracks_info):
    """
    ëŒ€ì‚¬ ì˜¤ë””ì˜¤ì™€ ë°°ê²½ìŒì„ ë¯¹ì‹±í•˜ëŠ” í•¨ìˆ˜
    dialogue_audio_path: í•©ì³ì§„ ëŒ€ì‚¬ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    background_tracks_info: [{audioPath, startTime, endTime, volume}] í˜•íƒœì˜ ë°°ê²½ìŒ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        print("ğŸµ ë°°ê²½ìŒ ë¯¹ì‹± ì‹œì‘...")

        # ëŒ€ì‚¬ ì˜¤ë””ì˜¤ ë¡œë“œ
        dialogue_audio = AudioSegment.from_mp3(dialogue_audio_path)
        dialogue_duration = len(dialogue_audio)
        print(f"ğŸ“Š ëŒ€ì‚¬ ì˜¤ë””ì˜¤ ê¸¸ì´: {dialogue_duration}ms")

        # ë°°ê²½ìŒì´ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not background_tracks_info:
            print("âš ï¸ ë°°ê²½ìŒì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return dialogue_audio_path

        # ê° ë°°ê²½ìŒ íŠ¸ë™ ì²˜ë¦¬
        for idx, track_info in enumerate(background_tracks_info):
            bg_audio_path = track_info.get('audioPath')
            start_time = track_info.get('startTime', 0)  # ms ë‹¨ìœ„
            end_time = track_info.get('endTime', dialogue_duration)  # ms ë‹¨ìœ„
            volume_adjust = track_info.get('volume', -10)  # dB ë‹¨ìœ„ (ê¸°ë³¸: -10dBë¡œ ë°°ê²½ìŒ ë³¼ë¥¨ ë‚®ì¶¤)

            print(f"ğŸ¼ ë°°ê²½ìŒ {idx + 1} ì²˜ë¦¬ ì¤‘...")
            print(f"   - ì‹œì‘: {start_time}ms, ì¢…ë£Œ: {end_time}ms, ë³¼ë¥¨: {volume_adjust}dB")

            # ë°°ê²½ìŒ ë¡œë“œ
            bg_audio = AudioSegment.from_file(bg_audio_path)

            # ë°°ê²½ìŒ ë³¼ë¥¨ ì¡°ì ˆ
            bg_audio = bg_audio + volume_adjust

            # í•„ìš”í•œ ê¸¸ì´ ê³„ì‚°
            required_duration = end_time - start_time

            # ë°°ê²½ìŒì´ í•„ìš”í•œ ê¸¸ì´ë³´ë‹¤ ì§§ìœ¼ë©´ ë°˜ë³µ
            if len(bg_audio) < required_duration:
                repeat_times = (required_duration // len(bg_audio)) + 1
                bg_audio = bg_audio * repeat_times

            # í•„ìš”í•œ ê¸¸ì´ë§Œí¼ ìë¥´ê¸°
            bg_audio = bg_audio[:required_duration]

            # í˜ì´ë“œ ì¸/ì•„ì›ƒ íš¨ê³¼ (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
            fade_duration = min(500, required_duration // 4)  # 500ms ë˜ëŠ” ì „ì²´ ê¸¸ì´ì˜ 1/4
            bg_audio = bg_audio.fade_in(fade_duration).fade_out(fade_duration)

            # ë°°ê²½ìŒì„ ì ì ˆí•œ ìœ„ì¹˜ì— ì˜¤ë²„ë ˆì´
            # start_time ìœ„ì¹˜ë¶€í„° bg_audioë¥¼ ë¯¹ì‹±
            dialogue_audio = dialogue_audio.overlay(bg_audio, position=start_time)
            print(f"âœ… ë°°ê²½ìŒ {idx + 1} ë¯¹ì‹± ì™„ë£Œ")

        # ìµœì¢… ë¯¹ì‹±ëœ ì˜¤ë””ì˜¤ ì €ì¥
        output_filename = f"mixed_{uuid4().hex}.mp3"
        output_path = os.path.join(settings.MEDIA_ROOT, 'audio', output_filename)
        dialogue_audio.export(output_path, format="mp3")

        print(f"âœ… ë°°ê²½ìŒ ë¯¹ì‹± ì™„ë£Œ: {output_path}")
        return output_path

    except Exception as e:
        print(f"âŒ ë°°ê²½ìŒ ë¯¹ì‹± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ëŒ€ì‚¬ ì˜¤ë””ì˜¤ ê²½ë¡œ ë°˜í™˜
        return dialogue_audio_path
    

import os
import requests
from django.utils import timezone
from django.core.files.base import ContentFile
from book.models import VoiceList, VoiceType
from elevenlabs import ElevenLabs

def sync_voices_with_type():
    """
    ElevenLabsì˜ User Voice / Default Voiceë¥¼ DBì— ë„£ê³ ,
    VoiceTypeë„ ì—°ê²°í•˜ë©° sample_audioê¹Œì§€ ì €ì¥
    """
    ELEVEN_API_KEY = os.getenv('ELEVEN_API_KEY')
    eleven_client = ElevenLabs(api_key=ELEVEN_API_KEY)
    
    try:
        print("ElevenLabs í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ:", eleven_client)

        # 1. VoiceType ìƒì„±
        user_voice_type, _ = VoiceType.objects.get_or_create(name="User Voice")
        default_voice_type, _ = VoiceType.objects.get_or_create(name="Default Voice")

        # 2. ëª¨ë“  ë³´ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        all_voices = eleven_client.voices.get_all().voices
        print(f"ì´ {len(all_voices)}ê°œ ë³´ì´ìŠ¤ ê°€ì ¸ì˜´")

        for v in all_voices:
            voice_id = getattr(v, "voice_id", None)
            if not voice_id:
                print(f"âš ï¸ voice_id ì—†ìŒ, ìŠ¤í‚µ: {getattr(v, 'name', 'unknown')}")
                continue

            # 3. DBì— ì €ì¥
            voice, created = VoiceList.objects.update_or_create(
                voice_id=voice_id,
                defaults={
                    "voice_name": getattr(v, "name", "Unknown"),
                    "voice_description": getattr(v, "description", ""),
                    "language_code": getattr(v, "language", "en"),
                    "created_at": timezone.now(),
                }
            )

            # 4. íƒ€ì… ì—°ê²°
            if getattr(v, "is_user", False):
                voice.types.add(user_voice_type)
            else:
                voice.types.add(default_voice_type)

            # 5. sample_audio ì €ì¥
            preview_url = getattr(v, "preview_url", None)
            if preview_url:
                try:
                    r = requests.get(preview_url, timeout=10)
                    if r.status_code == 200:
                        filename = f"{voice.voice_name}_{voice.voice_id}.mp3".replace(" ", "_")
                        voice.sample_audio.save(filename, ContentFile(r.content), save=True)
                except Exception as e:
                    print(f"âš ï¸ ìƒ˜í”Œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {voice.voice_name}, {e}")

            print(f"{'ìƒì„±' if created else 'ì—…ë°ì´íŠ¸'}: {voice.voice_name}")

        print("âœ… Voice sync ì™„ë£Œ")

    except Exception as e:
        print("âŒ Voice sync ì‹¤íŒ¨:", e)






from openai import OpenAI
from django.conf import settings
import os
from book.models import VoiceList, Books


def chat_with_character(book_id, message):
    """
    GROK(OpenAI Grok API) ê¸°ë°˜ ìºë¦­í„° ëŒ€í™” í•¨ìˆ˜
    """

    # ì±… ë‚´ìš© ë¡œë“œ
    try:
        book = Books.objects.get(id=book_id)
        book_info = book.description or ""
        character_name = book.character_name or "ìºë¦­í„°"
    except:
        book_info = ""
        character_name = "ìºë¦­í„°"

    # í”„ë¡¬í”„íŠ¸
    prompt = f"""
ë‹¹ì‹ ì€ '{character_name}'ë¼ëŠ” ìºë¦­í„°ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì±…ì˜ ì„¤ì •ì…ë‹ˆë‹¤:

{book_info}

ì‚¬ìš©ì ë©”ì‹œì§€ì— '{character_name}'ì˜ ë§íˆ¬ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”.
ë§íˆ¬ëŠ” ìºë¦­í„° ì„±ê²©ì— ë§ì¶”ê³ , ì¡´ëŒ“ë§/ë°˜ë§ì€ ìƒí™©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì„ íƒí•˜ì„¸ìš”.

ì‚¬ìš©ì: {message}
"""

    # --------------------------
    # ğŸ”¥ GROK í˜¸ì¶œ (grok_client)
    # --------------------------
    try:
        completion = grok_client.chat.completions.create(
            model="grok-4",   # ê·¸ë¡ ëª¨ë¸ ì´ë¦„(ë„£ì€ í‚¤ì— ë§ì¶° ë³€ê²½ ê°€ëŠ¥)
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì†Œì„¤ ì† ë“±ì¥ì¸ë¬¼ì²˜ëŸ¼ ë§í•˜ëŠ” ìºë¦­í„° AIì´ë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        ai_text = completion.choices[0].message.content

    except Exception as e:
        ai_text = f"[GROK ì˜¤ë¥˜] {str(e)}"

    # --------------------------
    # ğŸ”Š TTS ì²˜ë¦¬
    # --------------------------

    voice_id = "WAhoMTNdLdMoq1j3wf3I"
    language_code = "ko"

    audio_path = generate_tts(
        novel_text=ai_text,
        voice_id=voice_id,
        language_code=language_code,
        speed_value=1.0
    )

    # --------------------------
    # URL ë³€í™˜
    # --------------------------
    if audio_path and os.path.exists(str(audio_path)):
        rel_path = os.path.relpath(str(audio_path), settings.MEDIA_ROOT)
        audio_url = settings.MEDIA_URL + rel_path.replace("\\", "/")
    else:
        audio_url = None

    return {
        "text": ai_text,
        "audio": audio_url,
        "debug": {
            "ai_text": ai_text,
            "audio_path": str(audio_path),
            "audio_url": audio_url,
            "character": character_name,
            "source": "grok"
        }
    }



import json
from book.models import Books
# elevenlabs client import í•„ìš” ì‹œ ì¶”ê°€
# from elevenlabs import ElevenLabsClient
# eleven_client = ElevenLabsClient(api_key="YOUR_API_KEY")

def chat_with_character_debug(agent_id, character_name, voice_id, book_content, user_input):
    """
    ë””ë²„ê·¸ìš© ë˜í¼ í•¨ìˆ˜:
    ì‹¤ì œ ElevenLabs í˜¸ì¶œ ì—†ì´, TTS ë°˜í™˜ ëŒ€ì‹  debugìš© JSON ë°˜í™˜
    """
    if not agent_id or not character_name or not voice_id or not user_input:
        missing = []
        if not agent_id: missing.append("agent_id")
        if not character_name: missing.append("character_name")
        if not voice_id: missing.append("voice_id")
        if not user_input: missing.append("user_input")
        raise ValueError(f"ëˆ„ë½ëœ í•„ìˆ˜ í•„ë“œ: {', '.join(missing)}")

    # ì‹¤ì œ API í˜¸ì¶œì€ ì£¼ì„ ì²˜ë¦¬
    # response = eleven_client.conversational_ai.agents.chat(
    #     agent_id=agent_id,
    #     input=f"ìºë¦­í„° {character_name}, ë‚´ìš©: {book_content}, ì§ˆë¬¸: {user_input}",
    #     tts={"model_id": voice_id}
    # )

    # ë””ë²„ê·¸ìš© ë°˜í™˜
    debug_response = {
        "agent_id": agent_id,
        "character_name": character_name,
        "voice_id": voice_id,
        "book_content_snippet": book_content[:100] if book_content else "",
        "user_input": user_input,
        "audio": f"ì˜¤ë””ì˜¤_ë°ì´í„°_ì‹œë®¬ë ˆì´ì…˜_for_{character_name}"
    }

    return json.dumps(debug_response)


# ==================== ì„œë²„ ì‚¬ì´ë“œ WebAudio íš¨ê³¼ ====================
import numpy as np
from scipy.signal import butter, sosfilt

# 31ê°œ í”„ë¦¬ì…‹ íŒŒë¼ë¯¸í„° (webaudio-effects.jsì™€ 1:1 ë§¤ì¹­)
WEBAUDIO_PRESETS = {
    "normal": {"filter_type": "allpass", "freq": 1000, "Q": 1, "delay": 0, "feedback": 0, "tremolo_rate": 0, "tremolo_depth": 0},
    "phone": {"filter_type": "highpass", "freq": 2000, "Q": 8, "delay": 0, "feedback": 0, "tremolo_rate": 0, "tremolo_depth": 0},
    "cave": {"filter_type": "lowpass", "freq": 600, "Q": 6, "delay": 0.45, "feedback": 0.7, "tremolo_rate": 0, "tremolo_depth": 0},
    "underwater": {"filter_type": "lowpass", "freq": 400, "Q": 2, "delay": 0.15, "feedback": 0.3, "tremolo_rate": 5, "tremolo_depth": 0.6},
    "robot": {"filter_type": "highpass", "freq": 1200, "Q": 1, "delay": 0, "feedback": 0, "tremolo_rate": 30, "tremolo_depth": 1.0},
    "ghost": {"filter_type": "bandpass", "freq": 500, "Q": 9, "delay": 0.5, "feedback": 0.8, "tremolo_rate": 3, "tremolo_depth": 0.7},
    "child": {"filter_type": "allpass", "freq": 1500, "Q": 2, "delay": 0, "feedback": 0, "tremolo_rate": 15, "tremolo_depth": 0.3},
    "old": {"filter_type": "lowpass", "freq": 700, "Q": 3, "delay": 0.2, "feedback": 0.5, "tremolo_rate": 2, "tremolo_depth": 0.2},
    "echo": {"filter_type": "allpass", "freq": 1000, "Q": 1, "delay": 0.6, "feedback": 0.7, "tremolo_rate": 0, "tremolo_depth": 0},
    "protoss": {"filter_type": "allpass", "freq": 1100, "Q": 6, "delay": 0.09, "feedback": 0.42, "tremolo_rate": 0, "tremolo_depth": 0},
    "whisper": {"filter_type": "bandpass", "freq": 1800, "Q": 4, "delay": 0.03, "feedback": 0.2, "tremolo_rate": 4, "tremolo_depth": 0.4},
    "radio": {"filter_type": "bandpass", "freq": 1800, "Q": 2, "delay": 0, "feedback": 0, "tremolo_rate": 6.5, "tremolo_depth": 0.7},
    "megaphone": {"filter_type": "highpass", "freq": 900, "Q": 5, "delay": 0.05, "feedback": 0.35, "tremolo_rate": 0, "tremolo_depth": 0},
    "demon": {"filter_type": "lowpass", "freq": 800, "Q": 3, "delay": 0.07, "feedback": 0.6, "tremolo_rate": 120, "tremolo_depth": 0.9},
    "angel": {"filter_type": "highpass", "freq": 800, "Q": 5, "delay": 0.35, "feedback": 0.65, "tremolo_rate": 1.5, "tremolo_depth": 0.4},
    "vader": {"filter_type": "bandpass", "freq": 400, "Q": 8, "delay": 0.04, "feedback": 0.4, "tremolo_rate": 80, "tremolo_depth": 0.6},
    "giant": {"filter_type": "lowpass", "freq": 300, "Q": 4, "delay": 0.6, "feedback": 0.7, "tremolo_rate": 0, "tremolo_depth": 0},
    "tiny": {"filter_type": "highpass", "freq": 2200, "Q": 6, "delay": 0.02, "feedback": 0.3, "tremolo_rate": 8, "tremolo_depth": 0.4},
    "possessed": {"filter_type": "bandpass", "freq": 600, "Q": 5, "delay": 0.07, "feedback": 0.7, "tremolo_rate": 100, "tremolo_depth": 0.9},
    "horror": {"filter_type": "bandpass", "freq": 620, "Q": 14, "delay": 0.38, "feedback": 0.78, "tremolo_rate": 2.8, "tremolo_depth": 0.85},
    "helium": {"filter_type": "highpass", "freq": 2900, "Q": 7, "delay": 0.015, "feedback": 0.18, "tremolo_rate": 12, "tremolo_depth": 0.5},
    "timewarp": {"filter_type": "lowpass", "freq": 580, "Q": 9, "delay": 0.42, "feedback": 0.89, "tremolo_rate": 0.25, "tremolo_depth": 0.8},
    "glitch": {"filter_type": "bandpass", "freq": 1300, "Q": 22, "delay": 0.008, "feedback": 0.35, "tremolo_rate": 280, "tremolo_depth": 0.98},
    "choir": {"filter_type": "allpass", "freq": 1600, "Q": 5, "delay": 0.28, "feedback": 0.72, "tremolo_rate": 1.1, "tremolo_depth": 0.5},
    "hyperpop": {"filter_type": "highpass", "freq": 3200, "Q": 14, "delay": 0.018, "feedback": 0.42, "tremolo_rate": 220, "tremolo_depth": 0.9},
    "vaporwave": {"filter_type": "lowpass", "freq": 3400, "Q": 2, "delay": 0.38, "feedback": 0.78, "tremolo_rate": 0.35, "tremolo_depth": 0.8},
    "darksynth": {"filter_type": "bandpass", "freq": 950, "Q": 11, "delay": 0.24, "feedback": 0.70, "tremolo_rate": 130, "tremolo_depth": 0.55},
    "lofi-girl": {"filter_type": "lowpass", "freq": 4200, "Q": 1.8, "delay": 0.45, "feedback": 0.62, "tremolo_rate": 0.12, "tremolo_depth": 0.35},
    "bitcrush-voice": {"filter_type": "bandpass", "freq": 2200, "Q": 28, "delay": 0.004, "feedback": 0.25, "tremolo_rate": 420, "tremolo_depth": 0.98},
    "portal": {"filter_type": "allpass", "freq": 750, "Q": 18, "delay": 0.65, "feedback": 0.94, "tremolo_rate": 0.7, "tremolo_depth": 0.9},
    "neoncity": {"filter_type": "bandpass", "freq": 1150, "Q": 9, "delay": 0.52, "feedback": 0.80, "tremolo_rate": 2.8, "tremolo_depth": 0.45},
    "ghost-in-machine": {"filter_type": "bandpass", "freq": 780, "Q": 20, "delay": 0.09, "feedback": 0.58, "tremolo_rate": 190, "tremolo_depth": 0.88},
}


def _apply_biquad_filter(samples, sample_rate, filter_type, freq, Q):
    """scipy butter í•„í„°ë¡œ WebAudio BiquadFilter ì¬í˜„"""
    nyq = sample_rate / 2.0
    freq = min(freq, nyq - 1)

    if filter_type == "allpass":
        return samples  # allpass = í†µê³¼
    elif filter_type == "lowpass":
        sos = butter(2, freq / nyq, btype='low', output='sos')
    elif filter_type == "highpass":
        sos = butter(2, freq / nyq, btype='high', output='sos')
    elif filter_type == "bandpass":
        low = max(freq / (Q if Q > 0 else 1), 20) / nyq
        high = min(freq * (Q if Q > 0 else 1), nyq - 1) / nyq
        if low >= high:
            low = max(20 / nyq, 0.001)
            high = min(0.999, freq * 2 / nyq)
        sos = butter(2, [low, high], btype='band', output='sos')
    else:
        return samples

    return sosfilt(sos, samples).astype(np.float32)


def _apply_delay(samples, sample_rate, delay_time, feedback_gain, max_iterations=8):
    """ë”œë ˆì´ + í”¼ë“œë°± íš¨ê³¼"""
    if delay_time <= 0 and feedback_gain <= 0:
        return samples

    delay_samples = int(delay_time * sample_rate)
    if delay_samples <= 0:
        return samples

    output = samples.copy()
    delayed = samples.copy()

    for i in range(max_iterations):
        gain = feedback_gain ** (i + 1)
        if gain < 0.01:
            break
        padded = np.zeros(len(samples), dtype=np.float32)
        start = delay_samples * (i + 1)
        if start >= len(samples):
            break
        end = min(start + len(delayed), len(samples))
        padded[start:end] = delayed[:end - start] * gain
        output += padded

    # í´ë¦¬í•‘ ë°©ì§€
    max_val = np.max(np.abs(output))
    if max_val > 1.0:
        output = output / max_val
    return output


def _apply_tremolo(samples, sample_rate, rate, depth):
    """íŠ¸ë ˆëª°ë¡œ (AM ë³€ì¡°) íš¨ê³¼"""
    if rate <= 0 or depth <= 0:
        return samples

    t = np.arange(len(samples)) / sample_rate
    # depth 0~1: 0ì´ë©´ ë³€ì¡° ì—†ìŒ, 1ì´ë©´ ìµœëŒ€ ë³€ì¡°
    modulation = 1.0 - depth * 0.5 * (1.0 + np.sin(2 * np.pi * rate * t))
    return (samples * modulation).astype(np.float32)


def apply_webaudio_effect(audio_path, effect_name):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì— WebAudio í”„ë¦¬ì…‹ íš¨ê³¼ë¥¼ ì ìš©í•˜ì—¬ ìƒˆ íŒŒì¼ë¡œ ì €ì¥.
    webaudio-effects.jsì˜ 31ê°œ í”„ë¦¬ì…‹ì„ ì„œë²„ ì‚¬ì´ë“œë¡œ ì¬í˜„.

    Args:
        audio_path: MP3/WAV ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        effect_name: í”„ë¦¬ì…‹ ì´ë¦„ (e.g. "phone", "cave", "horror")

    Returns:
        ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (effectê°€ ì ìš©ëœ)
    """
    if effect_name == "normal" or effect_name not in WEBAUDIO_PRESETS:
        return audio_path

    preset = WEBAUDIO_PRESETS[effect_name]
    print(f"ğŸ›ï¸ WebAudio íš¨ê³¼ ì ìš©: {effect_name}")

    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = AudioSegment.from_file(audio_path)
        sample_rate = audio.frame_rate
        channels = audio.channels

        # numpy ë°°ì—´ë¡œ ë³€í™˜ (float32, -1~1 ë²”ìœ„)
        samples = np.array(audio.get_array_of_samples(), dtype=np.float32)
        samples = samples / (2 ** 15)  # 16bit â†’ float

        # ìŠ¤í…Œë ˆì˜¤ë©´ ëª¨ë…¸ë¡œ ì²˜ë¦¬ í›„ ë‹¤ì‹œ ìŠ¤í…Œë ˆì˜¤ë¡œ
        if channels == 2:
            left = samples[0::2]
            right = samples[1::2]
            # ì–‘ ì±„ë„ì— ë™ì¼ íš¨ê³¼ ì ìš©
            left = _process_channel(left, sample_rate, preset)
            right = _process_channel(right, sample_rate, preset)
            # ì¸í„°ë¦¬ë¸Œ
            samples = np.empty(len(left) + len(right), dtype=np.float32)
            samples[0::2] = left
            samples[1::2] = right
        else:
            samples = _process_channel(samples, sample_rate, preset)

        # float â†’ 16bit intë¡œ ë³€í™˜
        samples = np.clip(samples, -1.0, 1.0)
        samples_int = (samples * (2 ** 15 - 1)).astype(np.int16)

        # AudioSegmentë¡œ ì¬ì¡°ë¦½
        processed = AudioSegment(
            data=samples_int.tobytes(),
            sample_width=2,
            frame_rate=sample_rate,
            channels=channels
        )

        # ìƒˆ íŒŒì¼ë¡œ ì €ì¥
        output_filename = f"fx_{effect_name}_{uuid4().hex}.mp3"
        output_path = os.path.join(settings.MEDIA_ROOT, 'audio', output_filename)
        processed.export(output_path, format="mp3", bitrate="192k")

        print(f"âœ… WebAudio íš¨ê³¼ ì ìš© ì™„ë£Œ: {output_path}")
        return output_path

    except Exception as e:
        print(f"âŒ WebAudio íš¨ê³¼ ì ìš© ì˜¤ë¥˜ ({effect_name}): {e}")
        traceback.print_exc()
        return audio_path


def _process_channel(samples, sample_rate, preset):
    """ë‹¨ì¼ ì±„ë„ì— í•„í„° + ë”œë ˆì´ + íŠ¸ë ˆëª°ë¡œ ì²´ì¸ ì ìš©"""
    # 1. í•„í„° ì ìš©
    filtered = _apply_biquad_filter(
        samples, sample_rate,
        preset["filter_type"],
        preset["freq"],
        preset["Q"]
    )

    # 2. ë”œë ˆì´ ì ìš©
    delayed = _apply_delay(
        filtered, sample_rate,
        preset["delay"],
        preset["feedback"]
    )

    # 3. íŠ¸ë ˆëª°ë¡œ ì ìš©
    result = _apply_tremolo(
        delayed, sample_rate,
        preset["tremolo_rate"],
        preset["tremolo_depth"]
    )

    return result

